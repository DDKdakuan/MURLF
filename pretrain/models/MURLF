
from functools import partial

import torch
import torch.nn as nn

from timm.models.vision_transformer import Block

from timm.models.layers import DropPath, to_2tuple, trunc_normal_

from .util.pos_embed import get_2d_sincos_pos_embed


class PatchEmbed(nn.Module):
    """ Image to Patch Embedding
    """

    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):
        super().__init__()
        img_size = to_2tuple(img_size)
        patch_size = to_2tuple(patch_size)
        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])
        self.img_size = img_size
        self.patch_size = patch_size
        self.num_patches = num_patches

        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)
        self.norm = nn.LayerNorm(embed_dim)
        self.act = nn.GELU()

    def forward(self, x):
        B, C, H, W = x.shape
        # FIXME look at relaxing size constraints
        x = self.proj(x)
        Hp, Wp = x.shape[2], x.shape[3]
        x = self.norm(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)
        return self.act(x), (Hp, Wp)


class CMlp(nn.Module):

    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Conv2d(in_features, hidden_features, 1)
        self.act = act_layer()
        self.fc2 = nn.Conv2d(hidden_features, out_features, 1)
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return (x + x.mean(dim=1, keepdim=True)) * 0.5


class CBlock_S1(nn.Module):

    def __init__(self, dim, num_heads, mlp_ratio=4., drop=0., attn_drop=0.,
                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):
        super().__init__()
        self.norm1 = nn.LayerNorm(dim)
        self.conv1 = nn.Conv2d(dim, dim, 1)
        self.conv2 = nn.Conv2d(dim, dim, 1)
        self.attn = nn.Conv2d(dim, dim, 5, padding=2, groups=dim)

        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        self.norm2 = nn.LayerNorm(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = CMlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)

    def forward(self, x, mask=None):
        if mask is not None:
            x_masked = mask * self.conv1(self.norm1(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2))
            x = x + self.drop_path(self.conv2(self.attn(x_masked)))
        else:
            x_masked = self.conv1(self.norm1(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2))
            x = x + self.drop_path(self.conv2(self.attn(x_masked)))
        x = x + self.drop_path(self.mlp(self.norm2(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)))
        return x


class CBlock_S2(nn.Module):
    def __init__(self, dim, num_heads, mlp_ratio=4., drop=0., attn_drop=0.,
                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):
        super().__init__()
        self.norm1 = nn.LayerNorm(dim)
        self.conv1 = nn.Conv2d(dim, dim, 1)
        # self.conv2 = nn.Conv2d(dim, dim, 1)
        self.attn = nn.Conv2d(dim, dim, 5, padding=2, groups=dim)

        self.conv1d = nn.Conv1d(dim, dim, kernel_size=5, padding=2)

        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        self.norm2 = nn.LayerNorm(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = CMlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)

    def forward(self, x, mask=None):
        if mask is not None:
            x_masked = mask * self.conv1(self.norm1(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2))
            x_masked = self.attn(x_masked)

            # Reshape for Conv1d operation
            b, c, h, w = x_masked.shape
            x_masked = x_masked.view(b, c, h * w)  # Flatten spatial dimensions
            x_masked = self.conv1d(x_masked)  # Apply 1D convolution on channel dimension
            x_masked = x_masked.view(b, c, h, w)  # Reshape back to original spatial dimensions

            # Continue with the original operations
            x = x + self.drop_path(x_masked)
        else:

            x_masked = self.conv1(self.norm1(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2))
            x_masked = self.attn(x_masked)

            # Reshape for Conv1d operation
            b, c, h, w = x_masked.shape
            x_masked = x_masked.view(b, c, h * w)  # Flatten spatial dimensions
            x_masked = self.conv1d(x_masked)  # Apply 1D convolution on channel dimension
            x_masked = x_masked.view(b, c, h, w)  # Reshape back to original spatial dimensions

            # Continue with the original operations
            x = x + self.drop_path(x_masked)
        x = x + self.drop_path(self.mlp(self.norm2(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)))
        return x


class Masked_MJRLF(nn.Module):  # A multi-modal joint representation learning framework for remote sensing

    def __init__(self, img_size=224, patch_size=16, in_chans_model1=2, in_chans_model2=12,
                 embed_dim=1024, depth=24, num_heads=16,
                 decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,
                 mlp_ratio=4., norm_layer=nn.LayerNorm, norm_pix_loss=False):
        super().__init__()

        self.in_chans_model1 = in_chans_model1
        self.in_chans_model2 = in_chans_model2

        # 对于S1的embedding 4次
        self.patch_embed_s1_1 = PatchEmbed(
            img_size=img_size[0], patch_size=patch_size[0], in_chans=in_chans_model1, embed_dim=embed_dim[0])
        self.patch_embed_s1_2 = PatchEmbed(
            img_size=img_size[1], patch_size=patch_size[1], in_chans=embed_dim[0], embed_dim=embed_dim[1])
        self.patch_embed_s1_3 = PatchEmbed(
            img_size=img_size[2], patch_size=patch_size[2], in_chans=embed_dim[1], embed_dim=embed_dim[2])
        self.patch_embed_s1_4 = nn.Linear(embed_dim[2], embed_dim[2])

        # 对于S2的embedding 4次
        self.patch_embed_s2_1 = PatchEmbed(
            img_size=img_size[0], patch_size=patch_size[0], in_chans=in_chans_model2, embed_dim=embed_dim[0])
        self.patch_embed_s2_2 = PatchEmbed(
            img_size=img_size[1], patch_size=patch_size[1], in_chans=embed_dim[0], embed_dim=embed_dim[1])
        self.patch_embed_s2_3 = PatchEmbed(
            img_size=img_size[2], patch_size=patch_size[2], in_chans=embed_dim[1], embed_dim=embed_dim[2])
        self.patch_embed_s2_4 = nn.Linear(embed_dim[2], embed_dim[2])

        # 把不同尺度数据的维度统一
        self.stage1_output_decode_s1 = nn.Conv2d(embed_dim[0], embed_dim[2], 4, stride=4)
        self.stage2_output_decode_s1 = nn.Conv2d(embed_dim[1], embed_dim[2], 2, stride=2)

        self.stage1_output_decode_s2 = nn.Conv2d(embed_dim[0], embed_dim[2], 4, stride=4)
        self.stage2_output_decode_s2 = nn.Conv2d(embed_dim[1], embed_dim[2], 2, stride=2)

        num_patches_s1 = self.patch_embed_s1_3.num_patches
        num_patches_s2 = self.patch_embed_s2_3.num_patches

        # 两个模态位置编码
        self.pos_embed_s1 = nn.Parameter(torch.zeros(1, num_patches_s1, embed_dim[2]),
                                         requires_grad=False)  # fixed sin-cos embedding
        self.pos_embed_s2 = nn.Parameter(torch.zeros(1, num_patches_s2, embed_dim[2]),
                                         requires_grad=False)  # fixed sin-cos embedding

        # 这是关于S1分支部分
        self.blocks1_1 = nn.ModuleList([
            CBlock_S1(
                dim=embed_dim[0], num_heads=num_heads, mlp_ratio=mlp_ratio[0],
                norm_layer=norm_layer)
            for i in range(depth[0])])

        self.blocks1_2 = nn.ModuleList([
            CBlock_S1(
                dim=embed_dim[1], num_heads=num_heads, mlp_ratio=mlp_ratio[1], norm_layer=norm_layer)
            for i in range(depth[1])])

        # 这是关于S2分支部分
        self.blocks2_1 = nn.ModuleList([
            CBlock_S2(
                dim=embed_dim[0], num_heads=num_heads, mlp_ratio=mlp_ratio[0], norm_layer=norm_layer)
            for i in range(depth[0])])

        self.blocks2_2 = nn.ModuleList([
            CBlock_S2(
                dim=embed_dim[1], num_heads=num_heads, mlp_ratio=mlp_ratio[1], norm_layer=norm_layer)
            for i in range(depth[1])])

        self.blocks_s1_zong = nn.ModuleList([
            Block(
                dim=embed_dim[2], num_heads=num_heads, mlp_ratio=mlp_ratio[2], qkv_bias=True, norm_layer=norm_layer)
            for i in range(4)])
        self.blocks_s2_zong = nn.ModuleList([
            Block(
                dim=embed_dim[2], num_heads=num_heads, mlp_ratio=mlp_ratio[2], qkv_bias=True, norm_layer=norm_layer)
            for i in range(4)])

        self.blocks_zong = nn.ModuleList([
            Block(
                dim=embed_dim[2], num_heads=num_heads, mlp_ratio=mlp_ratio[2], qkv_bias=True, norm_layer=norm_layer)
            for i in range(depth[2])])
        self.norm_s1 = norm_layer(embed_dim[2])
        self.norm_s2 = norm_layer(embed_dim[2])
        # --------------------------------------------------------------------------

        # --------------------------------------------------------------------------
        # MAE decoder specifics
        self.decoder_embed_s1 = nn.Linear(embed_dim[-1], decoder_embed_dim, bias=True)
        self.decoder_embed_s2 = nn.Linear(embed_dim[-1], decoder_embed_dim, bias=True)

        self.mask_token_s1 = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))
        self.mask_token_s2 = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))

        self.decoder_pos_embed_s1 = nn.Parameter(torch.zeros(1, num_patches_s1, decoder_embed_dim),
                                                 requires_grad=False)  # fixed sin-cos embedding
        self.decoder_pos_embed_s2 = nn.Parameter(torch.zeros(1, num_patches_s2, decoder_embed_dim),
                                                 requires_grad=False)  # fixed sin-cos embedding

        self.decoder_blocks_s1 = nn.ModuleList([
            Block(decoder_embed_dim, decoder_num_heads, mlp_ratio[0], qkv_bias=True, norm_layer=norm_layer)
            for i in range(decoder_depth)])

        self.decoder_blocks_s2 = nn.ModuleList([
            Block(decoder_embed_dim, decoder_num_heads, mlp_ratio[0], qkv_bias=True, norm_layer=norm_layer)
            for i in range(decoder_depth)])

        self.decoder_norm_s1 = norm_layer(decoder_embed_dim)
        self.decoder_norm_s2 = norm_layer(decoder_embed_dim)

        self.decoder_pred_s1 = nn.Linear(decoder_embed_dim,
                                         (patch_size[0] * patch_size[1] * patch_size[2]) ** 2 * in_chans_model1,
                                         bias=True)  # decoder to patch
        self.decoder_pred_s2 = nn.Linear(decoder_embed_dim,
                                         (patch_size[0] * patch_size[1] * patch_size[2]) ** 2 * in_chans_model2,
                                         bias=True)  # decoder to patch

        # --------------------------------------------------------------------------
        self.initialize_weights()

    def initialize_weights(self):
        # initialization
        # initialize (and freeze) pos_embed by sin-cos embedding
        pos_embed_s1 = get_2d_sincos_pos_embed(self.pos_embed_s1.shape[-1],
                                               int(self.patch_embed_s1_3.num_patches ** .5), cls_token=False)
        self.pos_embed_s1.data.copy_(torch.from_numpy(pos_embed_s1).float().unsqueeze(0))

        pos_embed_s2 = get_2d_sincos_pos_embed(self.pos_embed_s2.shape[-1],
                                               int(self.patch_embed_s2_3.num_patches ** .5), cls_token=False)
        self.pos_embed_s2.data.copy_(torch.from_numpy(pos_embed_s2).float().unsqueeze(0))

        decoder_pos_embed_s1 = get_2d_sincos_pos_embed(self.decoder_pos_embed_s1.shape[-1],
                                                       int(self.patch_embed_s1_3.num_patches ** .5), cls_token=False)
        self.decoder_pos_embed_s1.data.copy_(torch.from_numpy(decoder_pos_embed_s1).float().unsqueeze(0))

        decoder_pos_embed_s2 = get_2d_sincos_pos_embed(self.decoder_pos_embed_s2.shape[-1],
                                                       int(self.patch_embed_s2_3.num_patches ** .5), cls_token=False)
        self.decoder_pos_embed_s2.data.copy_(torch.from_numpy(decoder_pos_embed_s2).float().unsqueeze(0))

        # initialize patch_embed like nn.Linear (instead of nn.Conv2d)
        w_s1 = self.patch_embed_s1_3.proj.weight.data
        torch.nn.init.xavier_uniform_(w_s1.view([w_s1.shape[0], -1]))

        w_s2 = self.patch_embed_s2_3.proj.weight.data
        torch.nn.init.xavier_uniform_(w_s2.view([w_s2.shape[0], -1]))

        # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)

        torch.nn.init.normal_(self.mask_token_s1, std=.02)
        torch.nn.init.normal_(self.mask_token_s2, std=.02)
        # initialize nn.Linear and nn.LayerNorm
        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            # we use xavier_uniform following official JAX ViT:
            torch.nn.init.xavier_uniform_(m.weight)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    def patchify(self, imgs, chanss):
        """
        imgs: (N, 3, H, W)
        x: (N, L, patch_size**2 *3)
        """
        p = 16

        assert imgs.shape[2] == imgs.shape[3] and imgs.shape[2] % p == 0

        h = w = imgs.shape[2] // p
        x = imgs.reshape(shape=(imgs.shape[0], chanss, h, p, w, p))
        x = torch.einsum('nchpwq->nhwpqc', x)
        x = x.reshape(shape=(imgs.shape[0], h * w, p ** 2 * chanss))
        return x

    def unpatchify(self, x, in_chans):
        """
        x: (N, L, patch_size**2 *3)
        imgs: (N, 3, H, W)
        """
        p = self.patch_embed.patch_size[0]
        h = w = int(x.shape[1] ** .5)
        assert h * w == x.shape[1]

        x = x.reshape(shape=(x.shape[0], h, w, p, p, in_chans))
        x = torch.einsum('nhwpqc->nchpwq', x)
        imgs = x.reshape(shape=(x.shape[0], in_chans, h * p, h * p))
        return imgs

    def random_masking(self, x, mask_ratio):
        """
        Perform per-sample random masking by per-sample shuffling.
        Per-sample shuffling is done by argsort random noise.
        x: [N, L, D], sequence
        """

        N = x.shape[0]

        L = self.patch_embed_s2_3.num_patches
        len_keep = int(L * (1 - mask_ratio))

        noise = torch.rand(N, L, device=x.device)  # noise in [0, 1]

        # sort noise for each sample
        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove
        ids_restore = torch.argsort(ids_shuffle, dim=1)

        # keep the first subset
        ids_keep = ids_shuffle[:, :len_keep]
        # x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))

        # generate the binary mask: 0 is keep, 1 is remove
        mask = torch.ones([N, L], device=x.device)
        mask[:, :len_keep] = 0
        # unshuffle to get the binary mask
        mask = torch.gather(mask, dim=1, index=ids_restore)

        return ids_keep, mask, ids_restore


    def forward_encoder(self, x, mask_ratio):

        x_s1 = x['s1']
        x_s2 = x['s2']

        # embed patches
        ids_keep_s1, mask_s1, ids_restore_s1 = self.random_masking(x_s1, mask_ratio)
        ids_keep_s2, mask_s2, ids_restore_s2 = self.random_masking(x_s2, mask_ratio)

        mask_for_patch1_s1 = mask_s1.reshape(-1, 14, 14).unsqueeze(-1).repeat(1, 1, 1, 16).reshape(-1, 14, 14, 4,
                                                                                                   4).permute(
            0, 1, 3, 2, 4).reshape(x_s1.shape[0], 56, 56).unsqueeze(1)
        mask_for_patch2_s1 = mask_s1.reshape(-1, 14, 14).unsqueeze(-1).repeat(1, 1, 1, 4).reshape(-1, 14, 14, 2,
                                                                                                  2).permute(
            0, 1, 3, 2, 4).reshape(x_s1.shape[0], 28, 28).unsqueeze(1)

        mask_for_patch1_s2 = mask_s2.reshape(-1, 14, 14).unsqueeze(-1).repeat(1, 1, 1, 16).reshape(-1, 14, 14, 4,
                                                                                                   4).permute(
            0, 1, 3, 2, 4).reshape(x_s2.shape[0], 56, 56).unsqueeze(1)
        mask_for_patch2_s2 = mask_s2.reshape(-1, 14, 14).unsqueeze(-1).repeat(1, 1, 1, 4).reshape(-1, 14, 14, 2,
                                                                                                  2).permute(
            0, 1, 3, 2, 4).reshape(x_s2.shape[0], 28, 28).unsqueeze(1)

        # stage 1
        x_s1, _ = self.patch_embed_s1_1(x_s1)
        # print('x_s1x_s1',x_s1.shape,type(x_s1))
        x_s2, _ = self.patch_embed_s2_1(x_s2)
        for blk in self.blocks1_1:
            x_s1 = blk(x_s1, 1 - mask_for_patch1_s1)
        for blk in self.blocks2_1:
            x_s2 = blk(x_s2, 1 - mask_for_patch1_s2)
        stage1_embed_s1 = self.stage1_output_decode_s1(x_s1).flatten(2).permute(0, 2, 1)
        stage1_embed_s2 = self.stage1_output_decode_s1(x_s2).flatten(2).permute(0, 2, 1)

        # stage 2
        x_s1, _ = self.patch_embed_s1_2(x_s1)
        x_s2, _ = self.patch_embed_s2_2(x_s2)
        for blk in self.blocks1_2:
            x_s1 = blk(x_s1, 1 - mask_for_patch2_s1)
        for blk in self.blocks2_2:
            x_s2 = blk(x_s2, 1 - mask_for_patch2_s2)
        stage2_embed_s1 = self.stage2_output_decode_s1(x_s1).flatten(2).permute(0, 2, 1)
        stage2_embed_s2 = self.stage2_output_decode_s2(x_s2).flatten(2).permute(0, 2, 1)

        # add pos embed w/o cls token
        x_s1, _ = self.patch_embed_s1_3(x_s1)
        x_s1 = x_s1.flatten(2).permute(0, 2, 1)
        x_s1 = self.patch_embed_s1_4(x_s1)
        x_s1 = x_s1 + self.pos_embed_s1
        x_s1 = torch.gather(x_s1, dim=1, index=ids_keep_s1.unsqueeze(-1).repeat(1, 1, x_s1.shape[-1]))

        x_s2, _ = self.patch_embed_s2_3(x_s2)
        x_s2 = x_s2.flatten(2).permute(0, 2, 1)
        x_s2 = self.patch_embed_s2_4(x_s2)
        x_s2 = x_s2 + self.pos_embed_s2
        x_s2 = torch.gather(x_s2, dim=1, index=ids_keep_s2.unsqueeze(-1).repeat(1, 1, x_s2.shape[-1]))

        # 前两步得到的特征图以同样的处理方式掩码
        stage1_embed_s1 = torch.gather(stage1_embed_s1, dim=1,
                                       index=ids_keep_s1.unsqueeze(-1).repeat(1, 1, stage1_embed_s1.shape[-1]))
        stage2_embed_s1 = torch.gather(stage2_embed_s1, dim=1,
                                       index=ids_keep_s1.unsqueeze(-1).repeat(1, 1, stage2_embed_s1.shape[-1]))

        stage1_embed_s2 = torch.gather(stage1_embed_s2, dim=1,
                                       index=ids_keep_s2.unsqueeze(-1).repeat(1, 1, stage1_embed_s2.shape[-1]))
        stage2_embed_s2 = torch.gather(stage2_embed_s2, dim=1,
                                       index=ids_keep_s2.unsqueeze(-1).repeat(1, 1, stage2_embed_s2.shape[-1]))

        # 对concat后的的x进行打乱
        x_s1_fenzhi = x_s1
        x_s2_fenzhi = x_s2

        x = torch.concatenate((x_s1, x_s2), dim=1)


        noise_x = torch.rand(x.shape[0], x.shape[1], device=x.device)
        ids_shuffle_x = torch.argsort(noise_x, dim=1)  # ascend: small is keep, large is remove
        x_s12 = torch.gather(x, dim=1, index=ids_shuffle_x.unsqueeze(-1).repeat(1, 1, x.shape[-1]))
        ids_restore_x = torch.argsort(ids_shuffle_x, dim=1)

        # append cls token

        # apply Transformer blocks
        # 打乱后进入encoder学习
        for blk in self.blocks_s1_zong:
            x_s1_fenzhi = blk(x_s1_fenzhi)

        for blk in self.blocks_s2_zong:
            x_s2_fenzhi = blk(x_s2_fenzhi)

        for blk in self.blocks_zong:
            x_s12 = blk(x_s12)

        # 将打乱后的x_s12恢复
        x_s12 = torch.gather(x_s12, dim=1, index=ids_restore_x.unsqueeze(-1).repeat(1, 1, x.shape[2]))

        L = self.patch_embed_s2_3.num_patches
        len_keep = int(L * (1 - mask_ratio))

        x_s1 = x_s12[:, :len_keep, :]
        x_s2 = x_s12[:, len_keep:, :]

        x_s1 = x_s1 + stage1_embed_s1 + stage2_embed_s1+x_s1_fenzhi
        x_s2 = x_s2 + stage1_embed_s2 + stage2_embed_s2+x_s2_fenzhi

        x_s1 = self.norm_s1(x_s1)
        x_s2 = self.norm_s2(x_s2)

        return x_s1, x_s2, mask_s1, mask_s2, ids_restore_s1, ids_restore_s2, ids_restore_x

    def forward_decoder(self, x_s1, x_s2, ids_restore_s1, ids_restore_s2, ids_restore_x, mask_ratio):
        # embed tokens

        x_s1 = self.decoder_embed_s1(x_s1)
        x_s2 = self.decoder_embed_s1(x_s2)

        # append mask tokens to sequence
        mask_tokens_s1 = self.mask_token_s1.repeat(x_s1.shape[0], ids_restore_s1.shape[1] - x_s1.shape[1], 1)
        mask_tokens_s2 = self.mask_token_s2.repeat(x_s2.shape[0], ids_restore_s2.shape[1] - x_s2.shape[1], 1)

        # 对s1进行扩充 mask 补全
        x_s1_ = torch.cat([x_s1, mask_tokens_s1], dim=1)  # no cls token
        x_deco_s1 = torch.gather(x_s1_, dim=1,
                                 index=ids_restore_s1.unsqueeze(-1).repeat(1, 1, x_s1.shape[2]))  # unshuffle

        # 对s2进行扩充 mask 补全
        x_s2_ = torch.cat([x_s2, mask_tokens_s2], dim=1)  # no cls token
        x_deco_s2 = torch.gather(x_s2_, dim=1,
                                 index=ids_restore_s2.unsqueeze(-1).repeat(1, 1, x_s2.shape[2]))  # unshuffle

        # 加入位置编码
        x_deco_s2 = x_deco_s2 + self.decoder_pos_embed_s2
        x_deco_s1 = x_deco_s1 + self.decoder_pos_embed_s1

        # apply Transformer blocks
        for blk in self.decoder_blocks_s1:
            x_deco_s1 = blk(x_deco_s1)
        x_deco_s1 = self.decoder_norm_s1(x_deco_s1)

        for blk in self.decoder_blocks_s2:
            x_deco_s2 = blk(x_deco_s2)
        x_deco_s2 = self.decoder_norm_s2(x_deco_s2)

        # predictor projection
        x_deco_s1 = self.decoder_pred_s1(x_deco_s1)
        x_deco_s2 = self.decoder_pred_s2(x_deco_s2)

        return x_deco_s1, x_deco_s2

    def forward_loss(self, imgs, pred_s1, pred_s2, mask_s1, mask_s2):
        """
        imgs: [N, 3, H, W]
        pred: [N, L, p*p*3]
        mask: [N, L], 0 is keep, 1 is remove,
        """
        target_s1 = self.patchify(imgs['s1'], chanss=self.in_chans_model1)
        target_s2 = self.patchify(imgs['s2'], chanss=self.in_chans_model2)

        # cal s1_loss
        mean_s1 = target_s1.mean(dim=-1, keepdim=True)
        var_s1 = target_s1.var(dim=-1, keepdim=True)
        target_s1 = (target_s1 - mean_s1) / (var_s1 + 1.e-6) ** .5
        loss_s1 = (pred_s1 - target_s1) ** 2
        loss_s1 = loss_s1.mean(dim=-1)  # [N, L], mean loss per patch [B,196]
        loss_s1 = (loss_s1 * mask_s1).sum() / mask_s1.sum()

        # cal s2_loss
        mean_s2 = target_s2.mean(dim=-1, keepdim=True)
        var_s2 = target_s2.var(dim=-1, keepdim=True)
        target_s2 = (target_s2 - mean_s2) / (var_s2 + 1.e-6) ** .5
        loss_s2 = (pred_s2 - target_s2) ** 2
        loss_s2 = loss_s2.mean(dim=-1)  # [N, L], mean loss per patch
        loss_s2 = (loss_s2 * mask_s2).sum() / mask_s2.sum()  # mean loss on removed patches

        loss = loss_s1 + loss_s2

        return loss

    def forward(self, imgs, mask_ratio=0.6):
        x_s1, x_s2, mask_s1, mask_s2, ids_restore_s1, ids_restore_s2, ids_restore_x = self.forward_encoder(imgs,
                                                                                                           mask_ratio)
        pred_s1, pred_s2 = self.forward_decoder(x_s1, x_s2, ids_restore_s1, ids_restore_s2, ids_restore_x,
                                                mask_ratio)  # [N, L, p*p*3]

        loss = self.forward_loss(imgs, pred_s1, pred_s2, mask_s1, mask_s2)

        return loss, pred_s1, pred_s2, mask_s1, mask_s2


def masked_mjrlf_patch16_dec512d8b(**kwargs):
    model = Masked_MJRLF(
        img_size=[224, 56, 28], patch_size=[4, 2, 2], in_chans_model1=2, in_chans_model2=10, embed_dim=[128, 192, 384],
        depth=[2, 2, 12], num_heads=6,
        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,
        mlp_ratio=[4, 4, 4], norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)
    return model

def masked_mjrlf_base_patch16_dec512d8b(**kwargs):
    model = Masked_MJRLF(
        img_size=[224, 56, 28], patch_size=[4, 2, 2], in_chans_model1=2, in_chans_model2=10, embed_dim=[256, 384, 768],
        depth=[2, 2, 12], num_heads=12,
        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,
        mlp_ratio=[4, 4, 4], norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)
    return model


# def mae_vit_base_patch16_dec512d8b(**kwargs):
#     model = MaskedAutoencoderViT(
#         patch_size=16, embed_dim=768, depth=12, num_heads=12,
#         decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,
#         mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)
#     return model
#
#
# def mae_vit_large_patch16_dec512d8b(**kwargs):
#     model = MaskedAutoencoderViT(
#         patch_size=16, embed_dim=1024, depth=24, num_heads=16,
#         decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,
#         mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)
#     return model
#
#
# def mae_vit_huge_patch14_dec512d8b(**kwargs):
#     model = MaskedAutoencoderViT(
#         patch_size=14, embed_dim=1280, depth=32, num_heads=16,
#         decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,
#         mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)
#     return model


# set recommended archs
mjrlf_small_patch16 = masked_mjrlf_patch16_dec512d8b   # decoder: 512 dim, 8 blocks
mjrlf_base_patch16 = masked_mjrlf_base_patch16_dec512d8b 

  # decoder: 512 dim, 8 blocks
# mae_vit_large_patch16 = mae_vit_large_patch16_dec512d8b  # decoder: 512 dim, 8 blocks
# mae_vit_huge_patch14 = mae_vit_huge_patch14_dec512d8b  # decoder: 512 dim, 8 blocks
# # new
# mae_vit_small_patch16 = mae_vit_small_patch16_dec512d8b  # decoder: 512 dim, 8 blocks
#
# aa = torch.rand(4, 2, 224, 224)
# bb = torch.rand(4, 12, 224, 224)
# input = {'s1': aa, 's2': bb}

# function1 = masked_mjrlf_patch16_dec512d8b()

# output = function1(input)
